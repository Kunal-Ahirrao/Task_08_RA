
# Research Task 08: Controlled Experiment on Bias Detection in LLM Narratives

This project, completed by **Kunal Ahirrao**, focuses on examining how identical datasets can lead to varying summaries depending on framing conditions applied to large language models (LLMs). The study aims to measure framing, demographic, and confirmation biases using a controlled experimental setup.

## Overview
The task involves designing a reproducible pipeline to evaluate bias in LLM-generated narratives. Multiple models, including GPT-4, Claude, and Gemini, were tested to compare variations in outputs under different prompt frames.

## Key Components
- **Python Scripts:** Automated prompt generation, data collection, and structured logging.
- **Datasets:** Anonymized text samples processed under different framing conditions.
- **Evaluation Metrics:** Bias categorization, linguistic variance, and sentiment polarity.
- **Documentation:** Includes methodology, results, and mitigation strategies in `REPORT.md`.

## Tools & Technologies
- Python (Pandas, NumPy, OpenAI API)
- GitHub for version control and transparency
- JSONL and CSV for structured logging and analysis
- Visualization using Matplotlib and Seaborn

## Ethical Compliance
All data was anonymized, and ethical considerations were followed throughout the study. The project adheres to data privacy and academic integrity standards.

## Outcome
This research provided insights into how prompt framing influences LLM outputs, highlighting the importance of transparency and bias mitigation in AI-generated narratives. It also strengthened skills in experimental design, automation, statistical analysis, and ethical AI evaluation.
